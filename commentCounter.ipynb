{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import pytagcloud\n",
    "import collections\n",
    "from konlpy.tag import Okt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import bigrams, word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from tqdm import tqdm\n",
    "from nltk import ConditionalFreqDist\n",
    "from nltk.probability import ConditionalProbDist, MLEProbDist\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "tagger = Okt()\n",
    "\n",
    "def tokenize(data):\n",
    "    tokens = ['/'.join(t) for t in tagger.pos(data)]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# totalBigram은 있어야 함, finalWord를 갱신하고 싶으면 여기부터 돌리고, 둘 다 있으면 밑으로 내려가서 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('total_bigram.json', 'r') as jsonFile:\n",
    "    totalBigram = json.load(jsonFile)\n",
    "#미리 저장한 totalBigram, 불러오는데 시간 좀 걸림\n",
    "\n",
    "totalTokens = [token[0] for token in totalBigram]\n",
    "#bigram에서 token 불러오기\n",
    "\n",
    "tokenCount = collections.Counter(totalTokens)\n",
    "sortedToken = sorted(tokenCount.items(), key=lambda x: x[1], reverse=True)\n",
    "sortedTokenDict = {}\n",
    "for i in range(len(sortedToken)):\n",
    "    sortedTokenDict[sortedToken[i][0]] = sortedToken[i][1]\n",
    "#토큰의 빈도 수로 만든 dictionary\n",
    "\n",
    "totalCfd = ConditionalFreqDist(totalBigram)\n",
    "#조건부확률 뭐시기인데, 이거 통해서 관련 단어 뽑아냄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def korean_most_common(c, n, pos=None):\n",
    "    if pos is None:\n",
    "        return totalCfd[tokenize(c)[0]].most_common(n)\n",
    "    else:\n",
    "        return totalCfd[\"/\".join([c, pos])].most_common(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = ['형/Noun', '형/Suffix', '형님/Noun', '이형/Noun','누나/Noun','언니/Suffix', '언니/Noun',\n",
    "             '대충/Noun', '브로/Noun', '브로앤팁스/Noun', '남자/Noun',\n",
    "             '대충/Noun', '얼굴/Noun', '피부/Noun', '제품/Noun', '트러블/Noun']\n",
    "\n",
    "#########################################중요 수정사항 많을 듯########################################\n",
    "\n",
    "def wordRatio(word):\n",
    "    kmc = korean_most_common(word, 1000)\n",
    "    test = {} ##비율이 들어갈 딕셔너리\n",
    "    for w in kmc:\n",
    "        try:\n",
    "            if int(sortedTokenDict[w[0]]) < 10:\n",
    "                pass\n",
    "            else:    \n",
    "                test[w[0]] = int(w[1])/int(sortedTokenDict[w[0]])\n",
    "        except:\n",
    "            continue\n",
    "    sort = sorted(test.items(), key=lambda x: x[1], reverse=True)[:30]\n",
    "    new = []\n",
    "    for t in sort:\n",
    "        if float(t[1]) < 0.1:\n",
    "            pass\n",
    "        else:\n",
    "            new.append(t)\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = []\n",
    "for word in word_list:\n",
    "    final.extend(wordRatio(word))\n",
    "    \n",
    "lastWord = []\n",
    "for w in final:\n",
    "    lastWord.append(w[0])    \n",
    "lastWord.extend(word_list)\n",
    "\n",
    "finalWord = list(set(lastWord))\n",
    "\n",
    "with open('finalWord.json', 'w', encoding = 'utf-8') as jsonFile:\n",
    "    json.dump(finalWord, jsonFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 여기서부터 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('53youtubers.csv', encoding = 'utf-8-sig')\n",
    "youtuber = list(df.youtuber[i] for i in range(len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('finalWord.json', 'r') as jsonFile:\n",
    "    finalWord = json.load(jsonFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalCount(youtuber):\n",
    "    tokens = []\n",
    "    df = pd.read_csv('{0}.csv'.format(youtuber), encoding = 'utf-8-sig', engine = 'python')\n",
    "    for i in tqdm(range(len(df))):\n",
    "        try:\n",
    "            tokens.extend(tokenize(df.text[i]))\n",
    "        except TypeError as e:\n",
    "            print(e)\n",
    "    finalToken = [w for w in tokens if w in finalWord]\n",
    "    finalCount = sorted(collections.Counter(finalToken).items(), key=lambda x: x[1], reverse=True)\n",
    "    return finalCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 19008/19008 [01:13<00:00, 257.81it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('형/Noun', 2069),\n",
       " ('제품/Noun', 211),\n",
       " ('얼굴/Noun', 182),\n",
       " ('형님/Noun', 160),\n",
       " ('남자/Noun', 140),\n",
       " ('이형/Noun', 110),\n",
       " ('형/Suffix', 82),\n",
       " ('누나/Noun', 67),\n",
       " ('대충/Noun', 22),\n",
       " ('명/Suffix', 21),\n",
       " ('언니/Noun', 19),\n",
       " ('언니/Suffix', 12),\n",
       " ('불매운동/Noun', 12),\n",
       " ('피부/Noun', 10),\n",
       " ('주접/Noun', 10),\n",
       " ('힘내/Verb', 7),\n",
       " ('보야/Adjective', 4),\n",
       " ('왁싱/Noun', 4),\n",
       " ('참으세요/Verb', 2),\n",
       " ('망했어/Adjective', 2),\n",
       " ('동차/Noun', 2),\n",
       " ('만나야지/Verb', 1),\n",
       " ('소멸/Noun', 1),\n",
       " ('살찐거/Adjective', 1),\n",
       " ('노화/Noun', 1),\n",
       " ('브로/Noun', 1),\n",
       " ('살자/Verb', 1),\n",
       " ('살쪘다/Adjective', 1),\n",
       " ('가봐도/Verb', 1),\n",
       " ('수배/Noun', 1),\n",
       " ('먹는거만/Verb', 1),\n",
       " ('일것이다/Verb', 1),\n",
       " ('만나고싶다/Verb', 1),\n",
       " ('수발/Noun', 1),\n",
       " ('감추는/Verb', 1),\n",
       " ('밝히는/Verb', 1),\n",
       " ('왔구나/Verb', 1),\n",
       " ('증맬루/Noun', 1),\n",
       " ('주름살/Noun', 1),\n",
       " ('뵙고싶습니다/Verb', 1),\n",
       " ('과로사/Noun', 1),\n",
       " ('솔직히말해/Adjective', 1),\n",
       " ('죽지마/Verb', 1),\n",
       " ('추해/Adjective', 1)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalCount('JM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
